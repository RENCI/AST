
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>processing package &#8212; AST  documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="utilities package" href="utilities.html" />
    <link rel="prev" title="harvester package" href="harvester.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="processing-package">
<h1>processing package<a class="headerlink" href="#processing-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-processing.compute_error_field">
<span id="processing-compute-error-field-module"></span><h2>processing.compute_error_field module<a class="headerlink" href="#module-processing.compute_error_field" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="processing.compute_error_field.combine_data_to_dict">
<code class="sig-prename descclassname">processing.compute_error_field.</code><code class="sig-name descname">combine_data_to_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">in_adc</span></em>, <em class="sig-param"><span class="n">in_obs</span></em>, <em class="sig-param"><span class="n">in_err</span></em>, <em class="sig-param"><span class="n">product</span><span class="o">=</span><span class="default_value">'WL'</span></em><span class="sig-paren">)</span> &#x2192; dict<a class="reference internal" href="_modules/processing/compute_error_field.html#combine_data_to_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.compute_error_field.combine_data_to_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Combines the three data frames into a single multiindex object
This is an optional procedure that maintains compatability with presently
executing data pipelines</p>
<dl class="simple">
<dt>Parameters:</dt><dd><p>in_adc,in_obs,in_diff:  DataFrames (datetime64 x station)
product: (str) A user selected product header name to add to final dict object</p>
</dd>
<dt>Returns:</dt><dd><p>df_merged_dict: A Dict of the merged 3 data source DataFrame suitable for JSON storage</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="processing.compute_error_field.compute_error_field">
<em class="property">class </em><code class="sig-prename descclassname">processing.compute_error_field.</code><code class="sig-name descname">compute_error_field</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obs</span></em>, <em class="sig-param"><span class="n">adc</span></em>, <em class="sig-param"><span class="n">meta</span></em>, <em class="sig-param"><span class="n">n_hours_per_period</span><span class="o">=</span><span class="default_value">12</span></em>, <em class="sig-param"><span class="n">n_hours_per_tide</span><span class="o">=</span><span class="default_value">12.42</span></em>, <em class="sig-param"><span class="n">n_pad</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">zthresh</span><span class="o">=</span><span class="default_value">3</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/processing/compute_error_field.html#compute_error_field"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.compute_error_field.compute_error_field" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This class supports computing station errors between observations and adcirc results.
Some facility for alignment of times, station exclusins, transformation to a semidiurnal time are supported.
The final results (errors) are packaged up for passage to an interpolator to build adcirc offset scaler felds.</p>
<p>Additional time-range seldction may be performed with bound_lo,bound_hi values. If no bounds are supplied, then the intersection of values between adcirc
and observations will be selected</p>
<p>The input data structures are generally expected to arise from using the AST/Harvester codes</p>
<dl>
<dt>Parameters:</dt><dd><blockquote>
<div><p>obs: A dataframe of index (datetime x stations) from observations (get_obs_stations)</p>
</div></blockquote>
<dl>
<dt>meta: A dataframe of meta data for the stations generally from calling the harvesterr code (get_obs_stations)</dt><dd><p>adc: A dataframe of index (datetime x stations) from, ASGS/ADCIRC (get_adc_stations)</p>
<p>All time series data are expected to come in as hourly sampling, though other sampling is supported</p>
<dl class="simple">
<dt>Case uses: Generally for ADDA or APSVIZ2, researchers want to assemble data into 12-hour periods and</dt><dd><p>examine the average error over each period. In this case one would set parameters as n_hours_per_periods=12, 
n_averging_periods=4.</p>
</dd>
</dl>
<p>For example, if you want to take a years worth of hourly data and return the daily averages for all data
one would: set n_hours_per_periods = 24, and n_averging_periods = n_yearly_length(hours) // 24</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="processing.compute_error_field.generate_merged_dict_data">
<code class="sig-prename descclassname">processing.compute_error_field.</code><code class="sig-name descname">generate_merged_dict_data</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">df_merged</span></em>, <em class="sig-param"><span class="n">product</span><span class="o">=</span><span class="default_value">'WL'</span></em><span class="sig-paren">)</span> &#x2192; dict<a class="reference internal" href="_modules/processing/compute_error_field.html#generate_merged_dict_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.compute_error_field.generate_merged_dict_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Reformat the df_merged data into a dict with Stations as the main key
Create the dict: self.df_merged_dict
For this class we can expect ADS/OBS/ERR data to all be available.
Must convert timestamp index to Strings YYYYMMDD HH:MM:SS</p>
<dl class="simple">
<dt>Parameters:</dt><dd><p>df_merged: DataFrame. A multi-index,combined 3-data source (adc,obs,diff) object 
product: (str) A user selected product name to add to final dict object</p>
</dd>
<dt>Returns:</dt><dd><p>df_merged_dict: A Dict of the merged 3 data source DataFrame suitable for JSON storage</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="processing.compute_error_field.interpolate_and_sample">
<code class="sig-prename descclassname">processing.compute_error_field.</code><code class="sig-name descname">interpolate_and_sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">diurnal_range</span></em>, <em class="sig-param"><span class="n">df_in</span></em><span class="sig-paren">)</span> &#x2192; pandas.core.frame.DataFrame<a class="reference internal" href="_modules/processing/compute_error_field.html#interpolate_and_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.compute_error_field.interpolate_and_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>This function take an input DataFrame (usually sampled on the hour) and transforms it to a semidiurnal 
time series based on the desired indexes assembled in durnal_range. The new diurnal_range entries are
combined with the given DataFrame entries and assigned a NaN value.  Data are sorted and duplicates removed.
The data are then interpolated using a default model of Linear. The desired diurnal_range of values are
then returned to the caller.</p>
<dl class="simple">
<dt>Parameters:</dt><dd><p>df_in: A DataFrame times (datetime64) x stations
diurnal_range:  list of datetime64</p>
</dd>
<dt>Returns:</dt><dd><p>df_out: DataFrame. (datetime64 x station) on a semidiurnal time sequence</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="processing.compute_error_field.main">
<code class="sig-prename descclassname">processing.compute_error_field.</code><code class="sig-name descname">main</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/processing/compute_error_field.html#main"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.compute_error_field.main" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="module-processing.interpolate_scaled_offset_field">
<span id="processing-interpolate-scaled-offset-field-module"></span><h2>processing.interpolate_scaled_offset_field module<a class="headerlink" href="#module-processing.interpolate_scaled_offset_field" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="processing.interpolate_scaled_offset_field.combine_datasets_for_interpolation">
<code class="sig-prename descclassname">processing.interpolate_scaled_offset_field.</code><code class="sig-name descname">combine_datasets_for_interpolation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">list_dfs</span></em><span class="sig-paren">)</span> &#x2192; pandas.core.frame.DataFrame<a class="reference internal" href="_modules/processing/interpolate_scaled_offset_field.html#combine_datasets_for_interpolation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.interpolate_scaled_offset_field.combine_datasets_for_interpolation" title="Permalink to this definition">¶</a></dt>
<dd><p>This method simply combines the provided list of DataFrames into a single DataFrame for passage to
the interpolation method. We use a list here, since, the caller may vary what data actually gets included
for interpolation. The list is unbound. Each dataframe MUST contain the headers:
(‘LON’,’LAT’,’VAL’)</p>
<dl class="simple">
<dt>Parameters:</dt><dd><p>list_dfs: list(dfs)</p>
</dd>
<dt>Returns:</dt><dd><p>df_combined: DataFrame containing only (‘LON’,’LAT’,’VAL’)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="processing.interpolate_scaled_offset_field.generic_grid">
<code class="sig-prename descclassname">processing.interpolate_scaled_offset_field.</code><code class="sig-name descname">generic_grid</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/processing/interpolate_scaled_offset_field.html#generic_grid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.interpolate_scaled_offset_field.generic_grid" title="Permalink to this definition">¶</a></dt>
<dd><p>Build 2D grid for testing the interpolation_model_transform.
this is a style=grid type data set</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>df: DataFrame with columns [‘LON’,’LAT’]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="processing.interpolate_scaled_offset_field.get_lon_lat_control_values">
<code class="sig-prename descclassname">processing.interpolate_scaled_offset_field.</code><code class="sig-name descname">get_lon_lat_control_values</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fname</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">header_data</span><span class="o">=</span><span class="default_value">'val'</span></em><span class="sig-paren">)</span> &#x2192; pandas.core.frame.DataFrame<a class="reference internal" href="_modules/processing/interpolate_scaled_offset_field.html#get_lon_lat_control_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.interpolate_scaled_offset_field.get_lon_lat_control_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Simply read the CSV file 
Ensure the columns are all have the names: LON,LAT,VALUE</p>
<p>land/water control data are always lon,lat,val.</p>
<dl>
<dt>Parameters:</dt><dd><p>fname: (str) Filename of the desired land/water control points
header_data: (str) Alternative name for the clamp/control node data header if not the default</p>
<blockquote>
<div><p>Will be changed to VAL on exit</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>df: DataFrame with the headers: LON,LAT,VAL</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="processing.interpolate_scaled_offset_field.get_station_values">
<code class="sig-prename descclassname">processing.interpolate_scaled_offset_field.</code><code class="sig-name descname">get_station_values</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fname</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">header_data</span><span class="o">=</span><span class="default_value">'mean'</span></em><span class="sig-paren">)</span> &#x2192; pandas.core.frame.DataFrame<a class="reference internal" href="_modules/processing/interpolate_scaled_offset_field.html#get_station_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.interpolate_scaled_offset_field.get_station_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Simply read a proper CSV file</p>
<p>station (averaging) files can be more complicated than control point files
but we assume stationids are on the index and that each has lon,lat,mean (or header_data)</p>
<p>The INPUT headers are very specific to AST usage
examples of typical header_data could be ‘mean’,’std’</p>
<dl>
<dt>Parameters:</dt><dd><p>fname: (str) Filename of the desired station (averaging) file
header_data: (str) Alternative name for the input desired values. Eg if not mean it might be “<a href="#id1"><span class="problematic" id="id2">Period_</span></a>-0”</p>
<blockquote>
<div><p>which by default is the mean of the most last period
Will be changed to VAL on exit</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>df: In the format STATIONID(index),LON,LAT,VAL</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="processing.interpolate_scaled_offset_field.interpolation_model_fit">
<code class="sig-prename descclassname">processing.interpolate_scaled_offset_field.</code><code class="sig-name descname">interpolation_model_fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">df_combined</span></em>, <em class="sig-param"><span class="n">fill_value</span><span class="o">=</span><span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">interpolation_type</span><span class="o">=</span><span class="default_value">'LinearNDInterpolator'</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/processing/interpolate_scaled_offset_field.html#interpolation_model_fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.interpolate_scaled_offset_field.interpolation_model_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Method to manage building interpolations of gauge (ADCIRC-Observational) errors (residuals)
The model is constructed by interpolation of the input error points
usually constrained with both zero-nodes positioned offcoast and land control points to better control
fluctuations at the coast.</p>
<dl class="simple">
<dt>The types of interpolation models that are supported are:</dt><dd><p>LinearNDInterpolator (default)
CloughTocher2DInterpolator</p>
</dd>
<dt>Parameters:</dt><dd><p>df_combined (DatraFrame) is the DataFrame with columns: LON,LAT,VAL that contains all 
source and clamp points. (DataFrame) It is expected the caller has already KNN processed the 
land_control points and concatenated the water_control points</p>
</dd>
<dt>Returns:</dt><dd><p>model: The actual model use for subsequent extrapolations</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="processing.interpolate_scaled_offset_field.interpolation_model_transform">
<code class="sig-prename descclassname">processing.interpolate_scaled_offset_field.</code><code class="sig-name descname">interpolation_model_transform</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">adc_combined</span></em>, <em class="sig-param"><span class="n">model</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_grid_type</span><span class="o">=</span><span class="default_value">'points'</span></em><span class="sig-paren">)</span> &#x2192; pandas.core.frame.DataFrame<a class="reference internal" href="_modules/processing/interpolate_scaled_offset_field.html#interpolation_model_transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.interpolate_scaled_offset_field.interpolation_model_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the precomputed model to the LON/LAT input points. These inputs usually
reflect the final grid of courtse such as an ADCIRC grid. The inputs can be of types points or grid.
If the LON/LAT lists are meant to be actual points to extrapolate on, then points. Else, if
that are INDEXES in the LON/LAT directions, then choose GRID.</p>
<dl class="simple">
<dt>Parameters:</dt><dd><p>adc_combined: dict with keys ‘LON’:list,’LAT’:list
input_grid_type: (str) either points or grid.
model: Resuting from a prior ‘fit’</p>
</dd>
<dt>Returns:</dt><dd><p>DataFrame: inteprolated grid</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="processing.interpolate_scaled_offset_field.knn_fit_control_points">
<code class="sig-prename descclassname">processing.interpolate_scaled_offset_field.</code><code class="sig-name descname">knn_fit_control_points</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">df_source</span></em>, <em class="sig-param"><span class="n">df_controls</span></em>, <em class="sig-param"><span class="n">nearest_neighbors</span><span class="o">=</span><span class="default_value">3</span></em><span class="sig-paren">)</span> &#x2192; pandas.core.frame.DataFrame<a class="reference internal" href="_modules/processing/interpolate_scaled_offset_field.html#knn_fit_control_points"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.interpolate_scaled_offset_field.knn_fit_control_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the values of input control_points based on the KNN(k) of the input df_source DataFrame
indata errors. Then apply that model to predict the control point values</p>
<dl class="simple">
<dt>This is setup to use a kd_tree method with uniform weights. The default metric is minkowski</dt><dd><p>with a default of using manhattan_distance (l1),</p>
</dd>
</dl>
<p>The input df_source data are double checked for missingness which must be removed</p>
<dl>
<dt>Parameters:</dt><dd><dl class="simple">
<dt>df_controls: DataFrame with the headers (‘LON’,’LAT’,’VAL’)</dt><dd><p>Usually read by get_lon_lat_control_values()</p>
</dd>
<dt>df_source: DataFrame with (at least) the headers (‘LON’,’LAT’,’VAL’)</dt><dd><p>Usually read by get_station_values()</p>
</dd>
</dl>
<p>nearest_neighbors: (int)  Number of neighbors to perform regression fit</p>
</dd>
<dt>Returns:</dt><dd><p>df_fit_controls: DatraFrame with the columns (‘LON’,’LAT’,’VAL’)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="processing.interpolate_scaled_offset_field.main">
<code class="sig-prename descclassname">processing.interpolate_scaled_offset_field.</code><code class="sig-name descname">main</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">args</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/processing/interpolate_scaled_offset_field.html#main"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.interpolate_scaled_offset_field.main" title="Permalink to this definition">¶</a></dt>
<dd><p>This reads pre-generated files containing station errors, land controls and water clamps. The land controls
have not been processed usng KNN.</p>
</dd></dl>

<dl class="py function">
<dt id="processing.interpolate_scaled_offset_field.randomly_select_dataframe_rows">
<code class="sig-prename descclassname">processing.interpolate_scaled_offset_field.</code><code class="sig-name descname">randomly_select_dataframe_rows</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">df</span></em>, <em class="sig-param"><span class="n">frac</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span> &#x2192; pandas.core.frame.DataFrame<a class="reference internal" href="_modules/processing/interpolate_scaled_offset_field.html#randomly_select_dataframe_rows"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.interpolate_scaled_offset_field.randomly_select_dataframe_rows" title="Permalink to this definition">¶</a></dt>
<dd><p>Intended for, but not restricted to, station (averaging) data
This is primarily used if the user wants to perform statistical testing on the
resulting interpolated fields. Setting frac==1 (default) returns the entire data set 
with rows shuffled (potentially removing some order biases)
Setting to &lt; 1, returns a subset of the data randomly selected.</p>
<p>This method is a placeholder for more sophisticated splitting/subsampling</p>
<dl class="simple">
<dt>Parameters:</dt><dd><p>df: DataFrame to reshuffle/resample
frac: (int) fraction of randomly selected rows to retain</p>
</dd>
<dt>Returns:</dt><dd><p>dataFrame</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="processing.interpolate_scaled_offset_field.test_interpolation_fit">
<code class="sig-prename descclassname">processing.interpolate_scaled_offset_field.</code><code class="sig-name descname">test_interpolation_fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">df_source</span></em>, <em class="sig-param"><span class="n">df_land_controls</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">df_water_controls</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cv_splits</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">nearest_neighbors</span><span class="o">=</span><span class="default_value">3</span></em>, <em class="sig-param"><span class="n">stratified_header_name</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/processing/interpolate_scaled_offset_field.html#test_interpolation_fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#processing.interpolate_scaled_offset_field.test_interpolation_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>df_source is the set of stationids and their values (errors)</p>
<p>If land_controls are provided then after the df_source training/testing splits, the KNN fits will be performed
on the full land_control set (training)</p>
<p>If df_water_controls are provided, they will be combined with the training set+(opt) land_controls prior to 
the interpolation fit</p>
<p>The MEAN Squared Error computation is only performed using the test set of stations. If we included the water_controls
that would down-bias the statistics (they are always zero and always correct). The land_controls are also leftout but 
tested separately</p>
<dl>
<dt>Parameters:</dt><dd><p>df_source (DataFrame) stationds x errors
df_land_controls: (DataFrame) stationsids x values(==0). Will be updated using KNN
df_water_controls: (DataFrame) stationsids x values(==0) 
cv_splits: (int) Type of Cross-validation splitting
nearest_neighbors: (int) How many gauges should be applied to KNN compute land_control nodes 
stratified_header_name: (str) Name iof an existing header form which to stratify the KFolds. No checking is made</p>
<blockquote>
<div><p>to enforce a minimum number of members to train and test</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>kf_dict: (dict) Statistical values for the full data set. Returns aveMSE, for each CV fold and overall best_cnt
kf_cntr_dict: (dict) Statistical values for the only the land control nodes. Returns aveCnrlMSE, for each CV fold and overall best_cnt</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-processing">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-processing" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">AST</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="harvester.html">harvester package</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">processing package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-processing.compute_error_field">processing.compute_error_field module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-processing.interpolate_scaled_offset_field">processing.interpolate_scaled_offset_field module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-processing">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utilities.html">utilities package</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="harvester.html" title="previous chapter">harvester package</a></li>
      <li>Next: <a href="utilities.html" title="next chapter">utilities package</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, AST: AdcircSupportTools: RENCI EDS.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.4.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/processing.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>